{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1344b23",
   "metadata": {},
   "source": [
    "# Testing to chunk the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd4deb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@, Vikram\n",
      "# ~ ecode. learning\n",
      "\n",
      "100 SQL\n",
      "Commands\n",
      "The Project Gutenberg eBook of A Journey to the Centre of the Earth\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost \n",
      "Country NCountry CSeries NamSeries Cod2015 [YR22016 [YR22017 [YR22018 [YR22019 [YR22020 [YR22021 [YR22022 [YR22023 [YR22024 [YR2\n",
      "Australia AUS Access to EG.CFT.AC 100 100 100 100 100 100 100 100 .. ..\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pickle\n",
    "#fetching the paths\n",
    "text_file_path=r\"D:\\AGI Projects\\MultiPDF RAG Pipeline\\artifacts\\loaded_data\\text_docs.pkl\"\n",
    "table_file_path=r\"D:\\AGI Projects\\MultiPDF RAG Pipeline\\artifacts\\loaded_data\\table_docs.pkl\"\n",
    "images_file_path=r\"D:\\AGI Projects\\MultiPDF RAG Pipeline\\artifacts\\loaded_data\\image_docs.pkl\"\n",
    "\n",
    "with open(text_file_path,'rb') as file:\n",
    "    text_data=pickle.load(file)\n",
    "with open(table_file_path,'rb') as file:\n",
    "    table_data=pickle.load(file)\n",
    "with open(images_file_path,'rb') as file:\n",
    "    images_data=pickle.load(file)\n",
    "\n",
    "print(images_data[0].page_content[:200])\n",
    "print(text_data[0].page_content[:200])\n",
    "print(table_data[0].page_content[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3371df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# start chunking the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ced362",
   "metadata": {},
   "source": [
    "# Chunking the data down now\n",
    "\n",
    "\n",
    "| Data Type   | Recommended Splitter                         | Notes                            |\n",
    "| ----------- | -------------------------------------------- | -------------------------------- |\n",
    "| Text-heavy  | `RecursiveCharacterTextSplitter`             | Works well out of box            |\n",
    "| Table-heavy | `CharacterTextSplitter` or **custom rows**   | Donâ€™t split mid-row              |\n",
    "| Image/OCR   | `RecursiveCharacterTextSplitter` with tweaks | Try code-aware or markdown-aware |\n",
    "| Code-heavy  | `MarkdownTextSplitter` or custom             | Preserves syntax boundaries      |\n",
    "\n",
    "\n",
    "since my ocr data is pretty much clean itself no weird noise anything as such going with Recursice character text splitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cbb00d",
   "metadata": {},
   "source": [
    "<h3>Chunking Text Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43239a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "#creating object\n",
    "text_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "#splititng the characters\n",
    "text_chunks=text_splitter.split_documents(text_data)\n",
    "# print(text_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0505a1b2",
   "metadata": {},
   "source": [
    "<h3>Chunking Table Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403a4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "table_splitter=CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "table_chunks=table_splitter.split_documents(table_data)\n",
    "# print(table_data[0].page_content[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89c523",
   "metadata": {},
   "source": [
    "<h3> Chunking Image Data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917707d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "image_splitter=RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\",\"\\n\",\" \",\"\"]\n",
    ")\n",
    "image_chunks=image_splitter.split_documents(images_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400ab1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
